{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/silvan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "questions = pd.read_csv(\"data/train_dataset.csv\", header=None, encoding=\"iso-8859-1\", sep=\";\")\n",
    "questions.columns = ['id', 'question', 'answer', 'topic']\n",
    "\n",
    "\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "questions['question'] = questions['question'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6417 samples, validate on 713 samples\n",
      "Epoch 1/2\n",
      "6417/6417 [==============================] - 2s 300us/sample - loss: 0.7976 - categorical_accuracy: 0.7290 - val_loss: 0.4151 - val_categorical_accuracy: 0.8752\n",
      "Epoch 2/2\n",
      "6417/6417 [==============================] - 2s 263us/sample - loss: 0.3291 - categorical_accuracy: 0.9001 - val_loss: 0.3419 - val_categorical_accuracy: 0.8892\n",
      "7130/7130 [==============================] - 0s 57us/sample - loss: 0.2285 - categorical_accuracy: 0.9355\n",
      "1783/1783 [==============================] - 0s 74us/sample - loss: 0.3702 - categorical_accuracy: 0.8833\n",
      "Train acc: 0.9354839\n",
      "Test accuracy: 0.8833427\n"
     ]
    }
   ],
   "source": [
    "X = questions.question\n",
    "y = questions.topic\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "train_posts = X_train\n",
    "train_tags = y_train\n",
    "\n",
    "test_posts = X_test\n",
    "test_tags = y_test\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)\n",
    "\n",
    "\n",
    "val_score = model.evaluate(x_train, y_train,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    "print('Train acc:',val_score[1] )\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv(\"data/crowdanswers.tsv\", encoding=\"ISO-8859-1\", delimiter=\"\\t\", na_filter=False)\n",
    "generated_questions.columns = ['id', 'question', 'answer', 'difficulty', 'opinion', 'factuality']\n",
    "\n",
    "labels = sorted(['science-technology', 'for-kids', 'video-games', 'sports', 'music'])\n",
    "\n",
    "generated_questions['question'].apply(clean_text)\n",
    "x_predict = tokenize.texts_to_matrix(generated_questions['question'])\n",
    "# result = model.predict(x_predict)\n",
    "result= model.predict_classes(x_predict, batch_size=1)\n",
    "#y_classes = result.argmax(axis=-1)\n",
    "predictated_labels = [labels[i] for i in result]\n",
    "# predicted_label = sorted(labels)[result]\n",
    "output = pd.DataFrame(data={\"id\":generated_questions[\"id\"], \"question\":generated_questions[\"question\"], \"answer\":generated_questions[\"answer\"], \"difficulty\":generated_questions[\"difficulty\"], \"opinion\":generated_questions[\"opinion\"], \"factuality\":generated_questions[\"factuality\"], \"topic\":predictated_labels})\n",
    "output.to_csv('generated.csv',encoding='utf-8', index=False, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import Sequential, load_model\n",
    "from tensorflow.python.keras.layers import Dense, Embedding, Dropout, Flatten, Activation\n",
    "from tensorflow.python.keras.layers import LSTM, SimpleRNN\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "from tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,recall_score,precision_score,f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow import set_random_seed\n",
    "import gc\n",
    "import os\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "EMBEDDING_SIZE=300\n",
    "WORDS_SIZE=8000\n",
    "INPUT_SIZE=100\n",
    "NUM_CLASSES=5\n",
    "EPOCHS=10\n",
    "\n",
    "questions = pd.read_csv(\"data/train_dataset.csv\", header=None, encoding=\"iso-8859-1\", sep=\";\")\n",
    "questions.columns = ['id', 'question', 'answer', 'topic']\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    return text\n",
    "\n",
    "questions['question'] = questions['question'].apply(clean_text)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "\n",
    "tokenizer = keras_text.Tokenizer(num_words=2000,char_level=False)\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "#tokenizer.num_words=WORDS_SIZE\n",
    "\n",
    "maxlen = 100\n",
    "## Tokkenizing train data and create matrix\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_train = pad_sequences(list_tokenized_train, padding='post', maxlen=maxlen)\n",
    "\n",
    "## Tokkenizing test data and create matrix\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test = pad_sequences(list_tokenized_test, padding='post', maxlen=maxlen)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "y_train = utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test = utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = KeyedVectors.load_word2vec_format('word2vec/GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20298\n",
      "20298\n"
     ]
    }
   ],
   "source": [
    "googlenews_w2v_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "\n",
    "for word,i in word_index.items():\n",
    "  if word in word2vec.vocab:\n",
    "    googlenews_w2v_matrix[i] = word2vec[word]\n",
    "\n",
    "print(len(googlenews_w2v_matrix))\n",
    "print(len(word_index)+1)\n",
    "embedding_layer = Embedding(len(word_index)+1,\n",
    "                            300,\n",
    "                            weights=[googlenews_w2v_matrix],\n",
    "                            input_length=INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(name='Word2Vec LSTM')\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(70))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7130 samples, validate on 1783 samples\n",
      "Epoch 1/10\n",
      "7130/7130 [==============================] - 25s 4ms/sample - loss: 1.4177 - categorical_accuracy: 0.4003 - val_loss: 1.3590 - val_categorical_accuracy: 0.4173\n",
      "Epoch 2/10\n",
      "7130/7130 [==============================] - 24s 3ms/sample - loss: 1.3838 - categorical_accuracy: 0.4111 - val_loss: 1.3578 - val_categorical_accuracy: 0.4173\n",
      "Epoch 3/10\n",
      "7130/7130 [==============================] - 24s 3ms/sample - loss: 1.3838 - categorical_accuracy: 0.4111 - val_loss: 1.3616 - val_categorical_accuracy: 0.4173\n",
      "Epoch 4/10\n",
      "7130/7130 [==============================] - 24s 3ms/sample - loss: 1.3810 - categorical_accuracy: 0.4111 - val_loss: 1.3584 - val_categorical_accuracy: 0.4173\n",
      "Epoch 5/10\n",
      "7130/7130 [==============================] - 24s 3ms/sample - loss: 1.3814 - categorical_accuracy: 0.4111 - val_loss: 1.3550 - val_categorical_accuracy: 0.4173\n",
      "Epoch 6/10\n",
      "7130/7130 [==============================] - 24s 3ms/sample - loss: 1.3829 - categorical_accuracy: 0.4111 - val_loss: 1.3564 - val_categorical_accuracy: 0.4173\n",
      "Epoch 7/10\n",
      "7130/7130 [==============================] - 24s 3ms/sample - loss: 1.3821 - categorical_accuracy: 0.4111 - val_loss: 1.3541 - val_categorical_accuracy: 0.4173\n",
      "Epoch 8/10\n",
      "3456/7130 [=============>................] - ETA: 12s - loss: 1.3767 - categorical_accuracy: 0.4152"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0c767acd55b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           verbose =1,shuffle=True)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x = x_train,\n",
    "          y = y_train,\n",
    "          validation_data = (x_test, y_test),\n",
    "          epochs = EPOCHS,\n",
    "          batch_size = 128,\n",
    "          verbose =1,shuffle=True)\n",
    "\n",
    "\n",
    "val_score = model.evaluate(x_train, y_train, batch_size=batch_size, verbose=1,)\n",
    "score = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_questions = pd.read_csv(\"data/crowdanswers.tsv\", encoding=\"ISO-8859-1\", delimiter=\"\\t\", na_filter=False)\n",
    "generated_questions.columns = ['id', 'question', 'answer', 'difficulty', 'opinion', 'factuality']\n",
    "\n",
    "labels = sorted(['science-technology', 'for-kids', 'video-games', 'sports', 'music'])\n",
    "\n",
    "list_tokenized = tokenizer.texts_to_sequences(generated_questions['question'])\n",
    "x_predict = pad_sequences(list_tokenized, padding='post', maxlen=maxlen)\n",
    "\n",
    "result= model.predict_classes(x_predict, batch_size=128)\n",
    "predictated_labels = [labels[i] for i in result]\n",
    "print(predictated_labels)\n",
    "# predicted_label = sorted(labels)[result]\n",
    "output = pd.DataFrame(data={\"id\":generated_questions[\"id\"], \"question\":generated_questions[\"question\"], \"answer\":generated_questions[\"answer\"], \"difficulty\":generated_questions[\"difficulty\"], \"opinion\":generated_questions[\"opinion\"], \"factuality\":generated_questions[\"factuality\"], \"topic\":predictated_labels})\n",
    "output.to_csv('data/classified.csv',encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic\n",
      "for-kids              3.000000\n",
      "music                 2.166667\n",
      "science-technology    1.789474\n",
      "sports                2.200000\n",
      "video-games           2.000000\n",
      "Name: opinion, dtype: float64\n",
      "None\n",
      "[(37, 0.9433163396162836), (5, 0.9314468790167526), (11, 0.9111800356038081), (12, 0.8820236760765787), (15, 0.8714451980365711), (28, 0.8340365238533526), (17, 0.8175684156270614), (22, 0.7946714734372502), (10, 0.7589563910030562), (29, 0.7262048357697336)]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "questions = pd.read_csv(\"data/classified.csv\",\n",
    "                          encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "user_features = questions[questions['id'] == 1].groupby(['topic'])['opinion'].mean()\n",
    "print(user_features)\n",
    "questions = questions.groupby(['id'])\n",
    "\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "\n",
    "friends = []\n",
    "for n, g in questions:\n",
    "    if n == 1:\n",
    "        continue\n",
    "    # when classification not works properly then some users have only 4 topics    \n",
    "    if len(g.groupby(['topic'])['opinion'].mean()) < 5:\n",
    "        continue\n",
    "    corr, p_value = pearsonr(user_features, g.groupby(['topic'])['opinion'].mean())\n",
    "    friends.append((n, corr))    \n",
    "        \n",
    "print(friends.sort(key=takeSecond, reverse=True))\n",
    "print(friends[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'stemmed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-2d98dd2d5062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.0025\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtvec_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemmed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtvec_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mweights_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4374\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4375\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'stemmed'"
     ]
    }
   ],
   "source": [
    "questions = pd.read_csv(\"data/classified.csv\",\n",
    "                          encoding=\"utf-8\", sep=\",\")\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(min_df=.0025, max_df=.1, stop_words='english')\n",
    "tvec_weights = tvec.fit_transform(questions.stemmed.dropna())\n",
    "weights = np.asarray(tvec_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': tvec.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "Name: tfsvector, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "questions = pd.read_csv(\"data/classified.csv\",\n",
    "                          encoding=\"utf-8\", sep=\",\")\n",
    "\n",
    "STEMMER = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer=STEMMER):\n",
    "    return [stemmer.stem(item) for item in tokens]\n",
    "    \n",
    "    \n",
    "def tokenizer(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return stem_tokens(tokens)\n",
    "    \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words='english')\n",
    "# assuming our text elements exist in a pandas dataframe `df` with\n",
    "# a column / feature name of `document`\n",
    "tfs = tfidf.fit_transform(questions['question'])\n",
    "questions['tfsvector'] = list(tfs.toarray())\n",
    "\n",
    "\n",
    "questions = questions.groupby(['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
